<p><img src="/images/llm-parameters/vast_library_with_countless_books.png" alt="" /></p>

<p>Let’s establish some fundamentals. As more LLM’s launch every week/month, a metric used to describe where they sit amonst their peers is they are an ‘x’ parameter language model.</p>

<h3 id="a-library-with-countless-books">A Library with Countless Books</h3>

<blockquote>
  <p>Models with more parameters can potentially learn more complex patterns and relationships in language.</p>
</blockquote>

<h2 id="lets-unpack-this">Let’s unpack this</h2>
<ul>
  <li>Parameters are the weights and biases within the neural network architecture of the LLM. These are the values that the model learns during training to capture patterns and relationships in the data.</li>
  <li>The number of parameters in an LLM typically refers to the total count of these learnable weights and biases. For example, when someone says an LLM has 70 billion parameters, it means there are 70 billion adjustable numerical values in the model.</li>
  <li>Parameters determine how the model processes and generates text. They encode the model’s understanding of language, including vocabulary, grammar, context, and various linguistic patterns.</li>
  <li>The quantity of parameters is often used as a rough measure of an LLM’s capacity and potential capabilities. Models with more parameters can potentially learn more complex patterns and relationships in language.</li>
  <li>Parameters are adjusted during the training process using optimization algorithms to minimize the difference between the model’s predictions and the actual target outputs.</li>
</ul>

<p>While a larger number of parameters can increase a model’s potential capacity, <em>it doesn’t necessarily guarantee better performance</em>. The quality of training data, model architecture, and training process also play crucial roles in determining an LLM’s effectiveness.</p>

